2017-04-29 00:50:40 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 00:50:40 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 00:50:40 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 00:50:40 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 00:50:40 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 00:50:40 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 00:50:40 [scrapy.core.engine] INFO: Spider opened
2017-04-29 00:50:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 00:50:40 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 00:50:40 [scrapy.core.engine] ERROR: Error while obtaining start requests
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\core\engine.py", line 127, in _next_request
    request = next(slot.start_requests)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 26, in start_requests
    for section, url in urls:
ValueError: too many values to unpack
2017-04-29 00:50:40 [scrapy.core.engine] INFO: Closing spider (finished)
2017-04-29 00:50:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 4, 29, 7, 50, 40, 235000),
 'log_count/DEBUG': 1,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'start_time': datetime.datetime(2017, 4, 29, 7, 50, 40, 230000)}
2017-04-29 00:50:40 [scrapy.core.engine] INFO: Spider closed (finished)
2017-04-29 00:53:06 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 00:53:06 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 00:53:06 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 00:53:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 00:53:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 00:53:06 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 00:53:06 [scrapy.core.engine] INFO: Spider opened
2017-04-29 00:53:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 00:53:06 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 00:53:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 00:53:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 00:53:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 01:01:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 74, in get_article_list
    article_list =	response.css('section.main-content').css('div.story-headline')
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 74, in get_article_list
    article_list =	response.css('section.main-content').css('div.story-headline')
  File "c:\python27\lib\bdb.py", line 49, in trace_dispatch
    return self.dispatch_line(frame)
  File "c:\python27\lib\bdb.py", line 68, in dispatch_line
    if self.quitting: raise BdbQuit
BdbQuit
2017-04-29 01:01:02 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 01:01:04 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2017-04-29 01:01:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 74, in get_article_list
    article_list =	response.css('section.main-content').css('div.story-headline')
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 74, in get_article_list
    article_list =	response.css('section.main-content').css('div.story-headline')
  File "c:\python27\lib\bdb.py", line 49, in trace_dispatch
    return self.dispatch_line(frame)
  File "c:\python27\lib\bdb.py", line 68, in dispatch_line
    if self.quitting: raise BdbQuit
BdbQuit
2017-04-29 01:01:04 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-04-29 01:01:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1015,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 79251,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 4, 29, 8, 1, 4, 229000),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 9,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/BdbQuit': 2,
 'start_time': datetime.datetime(2017, 4, 29, 7, 53, 6, 900000)}
2017-04-29 01:01:04 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-04-29 01:01:08 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 01:01:08 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 01:01:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 01:01:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 01:01:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 01:01:08 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 01:01:08 [scrapy.core.engine] INFO: Spider opened
2017-04-29 01:01:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 01:01:08 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 01:01:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 01:01:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 01:01:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 76, in get_article_list
    print(article.css('h3').extract_first() + '\n')
  File "c:\python27\lib\encodings\cp437.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character u'\u2019' in position 291: character maps to <undefined>
2017-04-29 01:01:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 01:01:10 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 76, in get_article_list
    print(article.css('h3').extract_first() + '\n')
  File "c:\python27\lib\encodings\cp437.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character u'\u2019' in position 333: character maps to <undefined>
2017-04-29 01:01:10 [scrapy.core.engine] INFO: Closing spider (finished)
2017-04-29 01:01:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1015,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 79338,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 4, 29, 8, 1, 10, 247000),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeEncodeError': 2,
 'start_time': datetime.datetime(2017, 4, 29, 8, 1, 8, 732000)}
2017-04-29 01:01:10 [scrapy.core.engine] INFO: Spider closed (finished)
2017-04-29 01:05:10 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 01:05:10 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 01:05:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 01:05:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 01:05:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 01:05:10 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 01:05:10 [scrapy.core.engine] INFO: Spider opened
2017-04-29 01:05:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 01:05:10 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 01:05:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 01:05:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 01:05:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 01:05:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 78, in get_article_list
    print(article_title + '\n')
  File "c:\python27\lib\encodings\cp437.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character u'\u2019' in position 50: character maps to <undefined>
2017-04-29 01:05:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 78, in get_article_list
    print(article_title + '\n')
  File "c:\python27\lib\encodings\cp437.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character u'\u2019' in position 56: character maps to <undefined>
2017-04-29 01:05:12 [scrapy.core.engine] INFO: Closing spider (finished)
2017-04-29 01:05:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1015,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 79280,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 4, 29, 8, 5, 12, 147000),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeEncodeError': 2,
 'start_time': datetime.datetime(2017, 4, 29, 8, 5, 10, 618000)}
2017-04-29 01:05:12 [scrapy.core.engine] INFO: Spider closed (finished)
2017-04-29 01:05:42 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 01:05:42 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 01:05:42 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 01:05:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 01:05:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 01:05:43 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 01:05:43 [scrapy.core.engine] INFO: Spider opened
2017-04-29 01:05:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 01:05:43 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 01:05:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 01:05:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 01:05:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 01:05:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 78, in get_article_list
    print(article_title)
  File "c:\python27\lib\encodings\cp437.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character u'\u2019' in position 56: character maps to <undefined>
2017-04-29 01:05:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 78, in get_article_list
    print(article_title)
  File "c:\python27\lib\encodings\cp437.py", line 12, in encode
    return codecs.charmap_encode(input,errors,encoding_map)
UnicodeEncodeError: 'charmap' codec can't encode character u'\u2019' in position 50: character maps to <undefined>
2017-04-29 01:05:43 [scrapy.core.engine] INFO: Closing spider (finished)
2017-04-29 01:05:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1015,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 79389,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 4, 29, 8, 5, 43, 668000),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/UnicodeEncodeError': 2,
 'start_time': datetime.datetime(2017, 4, 29, 8, 5, 43, 138000)}
2017-04-29 01:05:43 [scrapy.core.engine] INFO: Spider closed (finished)
2017-04-29 01:08:01 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 01:08:01 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 01:08:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 01:08:02 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 01:08:02 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 01:08:02 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 01:08:02 [scrapy.core.engine] INFO: Spider opened
2017-04-29 01:08:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 01:08:02 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 01:08:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 01:08:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 01:08:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 01:08:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 78, in get_article_list
    print(article_title)
LookupError: unknown encoding: cp65001
2017-04-29 01:08:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 78, in get_article_list
    print(article_title)
LookupError: unknown encoding: cp65001
2017-04-29 01:08:02 [scrapy.core.engine] INFO: Closing spider (finished)
2017-04-29 01:08:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1015,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 78972,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 4, 29, 8, 8, 2, 765000),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/LookupError': 2,
 'start_time': datetime.datetime(2017, 4, 29, 8, 8, 2, 181000)}
2017-04-29 01:08:02 [scrapy.core.engine] INFO: Spider closed (finished)
2017-04-29 01:08:56 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 01:08:56 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 01:08:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 01:08:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 01:08:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 01:08:56 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 01:08:56 [scrapy.core.engine] INFO: Spider opened
2017-04-29 01:08:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 01:08:56 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 01:08:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 01:08:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 01:13:51 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2017-04-29 01:13:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 75, in get_article_list
    for article in article_list:
  File "c:\python27\lib\bdb.py", line 53, in trace_dispatch
    return self.dispatch_return(frame, arg)
  File "c:\python27\lib\bdb.py", line 91, in dispatch_return
    if self.quitting: raise BdbQuit
BdbQuit
2017-04-29 01:13:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 01:13:51 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-04-29 01:13:51 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 01:13:51 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2017-04-29 01:13:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 34, in parse
    articles = self.get_article_list(response)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 75, in get_article_list
    for article in article_list:
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 75, in get_article_list
    for article in article_list:
  File "c:\python27\lib\bdb.py", line 49, in trace_dispatch
    return self.dispatch_line(frame)
  File "c:\python27\lib\bdb.py", line 68, in dispatch_line
    if self.quitting: raise BdbQuit
BdbQuit
2017-04-29 01:13:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1015,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 78920,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 4, 29, 8, 13, 51, 731000),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 2,
 'log_count/INFO': 10,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'spider_exceptions/BdbQuit': 2,
 'start_time': datetime.datetime(2017, 4, 29, 8, 8, 56, 458000)}
2017-04-29 01:13:51 [scrapy.core.engine] INFO: Spider closed (shutdown)
2017-04-29 13:12:14 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 13:12:14 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 13:12:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 13:12:15 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 13:12:15 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 13:12:15 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 13:12:15 [scrapy.core.engine] INFO: Spider opened
2017-04-29 13:12:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 13:12:15 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 13:12:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 13:12:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 13:12:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 13:12:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 60, in parse
    article_date = datetime.datetime.strptime(article["date"], '%Y-%m-%d')
TypeError: 'Selector' object has no attribute '__getitem__'
2017-04-29 13:12:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 60, in parse
    article_date = datetime.datetime.strptime(article["date"], '%Y-%m-%d')
TypeError: 'Selector' object has no attribute '__getitem__'
2017-04-29 13:12:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/news/politics/wp/2017/04/28/trump-now-agrees-with-the-majority-of-americans-he-wasnt-ready-to-be-president/> (referer: https://www.washingtonpost.com/politics/?nid=top_nav_politics)
2017-04-29 13:12:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/other-countries-are-still-trying-to-figure-out-what-trump-means-to-them/2017/04/28/01dbaa98-25fe-11e7-b503-9d616bd5a305_story.html> (referer: https://www.washingtonpost.com/world/?nid=menu_nav_world)
2017-04-29 13:12:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/news/politics/wp/2017/04/28/trump-now-agrees-with-the-majority-of-americans-he-wasnt-ready-to-be-president/> (referer: https://www.washingtonpost.com/politics/?nid=top_nav_politics)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 92, in retrieve_article
    article_date = response.xpath('//span[@itemprop=datePublished')
  File "c:\python27\lib\site-packages\scrapy\http\response\text.py", line 115, in xpath
    return self.selector.xpath(query, **kwargs)
  File "c:\python27\lib\site-packages\parsel\selector.py", line 207, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "c:\python27\lib\site-packages\parsel\selector.py", line 203, in xpath
    **kwargs)
  File "src\lxml\lxml.etree.pyx", line 1587, in lxml.etree._Element.xpath (src\lxml\lxml.etree.c:59353)
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__ (src\lxml\lxml.etree.c:171233)
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result (src\lxml\lxml.etree.c:170190)
ValueError: XPath error: Invalid predicate in //span[@itemprop=datePublished
2017-04-29 13:12:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/other-countries-are-still-trying-to-figure-out-what-trump-means-to-them/2017/04/28/01dbaa98-25fe-11e7-b503-9d616bd5a305_story.html> (referer: https://www.washingtonpost.com/world/?nid=menu_nav_world)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 92, in retrieve_article
    article_date = response.xpath('//span[@itemprop=datePublished')
  File "c:\python27\lib\site-packages\scrapy\http\response\text.py", line 115, in xpath
    return self.selector.xpath(query, **kwargs)
  File "c:\python27\lib\site-packages\parsel\selector.py", line 207, in xpath
    six.reraise(ValueError, ValueError(msg), sys.exc_info()[2])
  File "c:\python27\lib\site-packages\parsel\selector.py", line 203, in xpath
    **kwargs)
  File "src\lxml\lxml.etree.pyx", line 1587, in lxml.etree._Element.xpath (src\lxml\lxml.etree.c:59353)
  File "src\lxml\xpath.pxi", line 307, in lxml.etree.XPathElementEvaluator.__call__ (src\lxml\lxml.etree.c:171233)
  File "src\lxml\xpath.pxi", line 227, in lxml.etree._XPathEvaluatorBase._handle_result (src\lxml\lxml.etree.c:170190)
ValueError: XPath error: Invalid predicate in //span[@itemprop=datePublished
2017-04-29 13:12:16 [scrapy.core.engine] INFO: Closing spider (finished)
2017-04-29 13:12:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2155,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 165078,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 4, 29, 20, 12, 16, 417000),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 5,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/TypeError': 2,
 'spider_exceptions/ValueError': 2,
 'start_time': datetime.datetime(2017, 4, 29, 20, 12, 15, 331000)}
2017-04-29 13:12:16 [scrapy.core.engine] INFO: Spider closed (finished)
2017-04-29 13:12:48 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: agg)
2017-04-29 13:12:48 [scrapy.utils.log] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'agg.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['agg.spiders'], 'LOG_FILE': 'log', 'BOT_NAME': 'agg'}
2017-04-29 13:12:48 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-04-29 13:12:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-04-29 13:12:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-04-29 13:12:49 [scrapy.middleware] INFO: Enabled item pipelines:
['agg.pipelines.DownloadPDFS',
 'agg.pipelines.AggPipeline',
 'agg.pipelines.JsonPipeline']
2017-04-29 13:12:49 [scrapy.core.engine] INFO: Spider opened
2017-04-29 13:12:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-04-29 13:12:49 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-04-29 13:12:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/robots.txt> (referer: None)
2017-04-29 13:12:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
2017-04-29 13:12:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
2017-04-29 13:12:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/politics/?nid=top_nav_politics> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 60, in parse
    article_date = datetime.datetime.strptime(article["date"], '%Y-%m-%d')
TypeError: 'Selector' object has no attribute '__getitem__'
2017-04-29 13:12:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/?nid=menu_nav_world> (referer: None)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 60, in parse
    article_date = datetime.datetime.strptime(article["date"], '%Y-%m-%d')
TypeError: 'Selector' object has no attribute '__getitem__'
2017-04-29 13:12:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/news/politics/wp/2017/04/28/trump-now-agrees-with-the-majority-of-americans-he-wasnt-ready-to-be-president/> (referer: https://www.washingtonpost.com/politics/?nid=top_nav_politics)
2017-04-29 13:12:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.washingtonpost.com/world/other-countries-are-still-trying-to-figure-out-what-trump-means-to-them/2017/04/28/01dbaa98-25fe-11e7-b503-9d616bd5a305_story.html> (referer: https://www.washingtonpost.com/world/?nid=menu_nav_world)
2017-04-29 22:19:18 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2017-04-29 22:19:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/news/politics/wp/2017/04/28/trump-now-agrees-with-the-majority-of-americans-he-wasnt-ready-to-be-president/> (referer: https://www.washingtonpost.com/politics/?nid=top_nav_politics)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 97, in retrieve_article
    item = response.meta["item"]
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 97, in retrieve_article
    item = response.meta["item"]
  File "c:\python27\lib\bdb.py", line 49, in trace_dispatch
    return self.dispatch_line(frame)
  File "c:\python27\lib\bdb.py", line 68, in dispatch_line
    if self.quitting: raise BdbQuit
BdbQuit
2017-04-29 22:19:18 [scrapy.core.engine] INFO: Closing spider (shutdown)
2017-04-29 22:19:19 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2017-04-29 22:19:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.washingtonpost.com/world/other-countries-are-still-trying-to-figure-out-what-trump-means-to-them/2017/04/28/01dbaa98-25fe-11e7-b503-9d616bd5a305_story.html> (referer: https://www.washingtonpost.com/world/?nid=menu_nav_world)
Traceback (most recent call last):
  File "c:\python27\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 97, in retrieve_article
    item = response.meta["item"]
  File "C:\Users\Ankit\Documents\crawler_project\crawler\agg\spiders\washpos.py", line 97, in retrieve_article
    item = response.meta["item"]
  File "c:\python27\lib\bdb.py", line 49, in trace_dispatch
    return self.dispatch_line(frame)
  File "c:\python27\lib\bdb.py", line 68, in dispatch_line
    if self.quitting: raise BdbQuit
BdbQuit
2017-04-29 22:19:19 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
